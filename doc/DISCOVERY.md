# The Discovery Stage

Why doesn't the workshop start with directly creating a judge or rubric question? Prior to this experience, 
it's likely that a group of informed stakeholders hasn't explored this topic of AI quality in a structured,
systematic way. Generic measurements like correctness, groundedness, etc. need to be defined in terms of specific business knowledge to make them into a truly useful measurement system.

## Goals of the discovery process

The discovery process helps elicit the raw material that can be refined into rubric questions. Participants will investigate individual examples to identify what constitutes high or low quality independently. The findings from each examples are then synthesized across examples and participants to come up with global definitions of quality.

During the annotation process, we test empirically whether all participants can agree on these global definitions. 


## Facilitating Discovery: Challenges + Solutions

The process can be more art than science. It's often messy, and can suffer from sparse responses, unclear expectations and cognitive overload. The facilitation experience in the application is designed to solve these problems: 

    - The application generates follow up questions specific to participants / examples. Observations from other participants may be included to specifically probe for interesting disagreements.
    - The findings aggregated by participant/example are presented to the facilitator to _inform_ discussion (not to replace it). Since facilitators often don't know about the domain, this helps reduce cognitive load.
    - Participants autonomously own the process with the faciliator and workshop just providing the framework. They will organically identify common clusters of findings and themes. 
