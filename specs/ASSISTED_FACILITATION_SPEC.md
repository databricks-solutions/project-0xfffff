# Assisted Facilitation Specification

## Overview

Assisted Facilitation helps stakeholders identify what constitutes high/low quality AI responses for their specific domain. During the Discovery phase, participants examine traces and submit findings which are automatically classified into structured categories. Facilitators use this structured view to guide discussions and promote findings into draft rubric elements.

### Goals

1. Surface raw material (findings) that can be refined into rubric questions
2. Provide facilitators with structured visibility into participant observations without requiring domain expertise
3. Bridge discovery insights directly into rubric creation through a promotion workflow

---

## Core Concepts

### Discovery Categories

Every finding is classified into exactly one category:

| Category | Description | Example |
|----------|-------------|---------|
| `themes` | Major patterns of quality or issues | "Response lacks source citations" |
| `edge_cases` | Unusual scenarios or boundary inputs | "Behavior is inconsistent when dates span timezones" |
| `boundary_conditions` | What separates quality levels | "Acceptable but would be better with primary sources" |
| `failure_modes` | Ways the response can fail | "Hallucinates when context is ambiguous" |
| `missing_info` | Information gaps or missing context | "No mention of limitations or caveats" |

### Disagreements

Disagreements are automatically detected when participants submit conflicting observations about the same trace. They are surfaced separately to facilitators to drive discussion.

### Promotion

Facilitators can "promote" findings to a draft rubric staging area. A promoted finding becomes raw material for rubric construction:

- "Temporal context should accompany temporal questions" → potential grading criterion
- "Acceptable but would be better with primary sources" → defines Likert scale boundary

---

## Participant Experience

### What Participants See

1. **Trace view** with input/output content
2. **Questions** to answer about the trace:
   - Q1 (always present): "What makes this response effective or ineffective?"
   - Q2+ (if generated by facilitator): Targeted follow-up questions
3. **Fuzzy progress indicator** showing overall workshop progress
   - Does NOT show category-level breakdown
   - Does NOT show per-trace detail
   - Prevents biasing findings by revealing what's "missing"

### Participant Actions

- Submit findings (free-form text answering the displayed questions)
- Navigate between traces
- Mark discovery complete when finished

---

## Facilitator Experience

### Per-Trace Structured View

For each trace, facilitators see:

```
Trace 1                                        [Generate Question]
├── themes          ████░░  2/3    [finding₁] [finding₂]
├── edge_cases      ██░░░░  1/3    [finding₃]
├── boundary_cond   ░░░░░░  0/3
├── failure_modes   ████░░  2/3    [finding₄] [finding₅]
├── missing_info    ░░░░░░  0/3
└── disagreements   █░░░░░  1/?    [Alice vs Bob: "..."]

Each finding shows: user attribution, text, [Promote] button
```

### Progress Bars

- Each category shows `count / threshold`
- Thresholds are configurable per trace (default: 3 per category)
- Facilitators adjust thresholds based on trace complexity

### Facilitator Actions

1. **Generate Question**: Creates a follow-up question for a trace, targeting gaps in coverage. Broadcasts to all participants viewing that trace.

2. **Promote Finding**: Sends a finding to the draft rubric staging area for later use in rubric composition.

3. **Adjust Thresholds**: Change "good enough" count per category per trace.

---

## Classification

### When Classification Happens

Classification occurs in real-time when a participant submits a finding.

### Classification Process

1. Participant submits finding text
2. LLM classifies into one of 5 categories (themes, edge_cases, boundary_conditions, failure_modes, missing_info)
3. Finding is stored with assigned category
4. Disagreement detection runs against other findings for the same trace
5. Facilitator view updates

### Disagreement Detection

After each finding submission, compare against other findings for the same trace:
- If conflicting viewpoints detected, create a Disagreement record
- Disagreement includes: participating users, summary of conflict, source finding IDs

---

## Question Generation

### Fixed Question (Q1)

Every trace has Q1: "What makes this response effective or ineffective?"

### Generated Questions (Q2+)

Facilitator triggers generation per trace. The question:
- Targets categories with gaps (below threshold)
- Probes unresolved disagreements if present
- Broadcasts to ALL participants on that trace (not per-user)

### Generation Logic

```
1. Check coverage: which categories are below threshold?
2. Check disagreements: any unresolved conflicts?
3. If disagreements exist and not yet probed → generate disagreement question
4. Else → generate question targeting lowest-coverage category
5. Store question at trace level
6. All participants see new question on next load/refresh
```

---

## Data Model

### TraceDiscoveryState

```python
class TraceDiscoveryState:
    trace_id: str
    workshop_id: str

    # Findings by category
    themes: List[ClassifiedFinding]
    edge_cases: List[ClassifiedFinding]
    boundary_conditions: List[ClassifiedFinding]
    failure_modes: List[ClassifiedFinding]
    missing_info: List[ClassifiedFinding]

    # Auto-detected disagreements
    disagreements: List[Disagreement]

    # Questions (Q1 fixed, Q2+ generated)
    questions: List[DiscoveryQuestion]

    # Configurable per-category thresholds
    thresholds: Dict[str, int]  # default: 3 per category
```

### ClassifiedFinding

```python
class ClassifiedFinding:
    id: str
    trace_id: str
    user_id: str
    text: str
    category: str  # themes | edge_cases | boundary_conditions | failure_modes | missing_info
    question_id: str  # Which question this answered
    promoted: bool
    created_at: datetime
```

### Disagreement

```python
class Disagreement:
    id: str
    trace_id: str
    user_ids: List[str]  # Participants who disagree
    finding_ids: List[str]  # The conflicting findings
    summary: str  # LLM-generated description
    created_at: datetime
```

### DiscoveryQuestion

```python
class DiscoveryQuestion:
    id: str  # q_1, q_2, etc.
    trace_id: str
    prompt: str
    placeholder: Optional[str]
    target_category: Optional[str]  # Category this question targets
    is_fixed: bool  # True for Q1
    created_at: datetime
```

### DraftRubricItem

```python
class DraftRubricItem:
    id: str
    source_finding_id: str
    source_trace_id: str
    workshop_id: str
    text: str
    promoted_by: str  # Facilitator user_id
    promoted_at: datetime
```

---

## API Endpoints

### Participant Endpoints

| Method | Path | Description |
|--------|------|-------------|
| GET | `/workshops/{id}/traces/{trace_id}/discovery-questions` | Get questions for trace |
| POST | `/workshops/{id}/findings` | Submit finding |
| GET | `/workshops/{id}/discovery-progress` | Get fuzzy global progress |

### Facilitator Endpoints

| Method | Path | Description |
|--------|------|-------------|
| GET | `/workshops/{id}/traces/{trace_id}/discovery-state` | Get full structured state |
| POST | `/workshops/{id}/traces/{trace_id}/generate-question` | Generate and broadcast question |
| PUT | `/workshops/{id}/traces/{trace_id}/thresholds` | Update thresholds |
| POST | `/workshops/{id}/findings/{finding_id}/promote` | Promote to draft rubric |
| GET | `/workshops/{id}/draft-rubric` | Get promoted findings |

---

## Success Criteria

1. Findings are classified in real-time as participants submit them
2. Facilitators see per-trace structured view with category breakdown
3. Facilitators can generate targeted questions that broadcast to all participants
4. Disagreements are auto-detected and surfaced
5. Participants see only fuzzy progress (no category bias)
6. Findings can be promoted to draft rubric staging area
7. Thresholds are configurable per category per trace
