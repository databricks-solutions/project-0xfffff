[
  {
    "input": "Review this function and suggest improvements:\n```python\ndef process(d):\n    r = []\n    for i in d:\n        if i > 0:\n            r.append(i * 2)\n    return r\n```",
    "output": "Here are some improvements for readability and clarity:\n\n```python\ndef double_positive_numbers(numbers: list[int]) -> list[int]:\n    \"\"\"Return a list of positive numbers doubled.\"\"\"\n    return [num * 2 for num in numbers if num > 0]\n```\n\nChanges made:\n- Descriptive function name (`double_positive_numbers` vs `process`)\n- Type hints for better documentation\n- Docstring explaining the purpose\n- List comprehension for conciseness\n- Meaningful variable names",
    "context": {
      "target_categories": [
        "themes"
      ],
      "difficulty": "easy",
      "rationale": "Highlights general code quality themes like naming, documentation, and readability",
      "source": "synthetic-discovery-generator",
      "generated_at": "2026-01-08T10:51:57.526526"
    }
  },
  {
    "input": "This JSON parser breaks on some inputs. Can you fix it?\n```python\nimport json\n\ndef parse_config(config_str):\n    return json.loads(config_str)\n```",
    "output": "The function doesn't handle edge cases well. Here's an improved version:\n\n```python\nimport json\nfrom typing import Any\n\ndef parse_config(config_str: str) -> dict[str, Any]:\n    \"\"\"Parse a JSON configuration string.\"\"\"\n    if not config_str:\n        return {}\n    \n    # Handle BOM (byte order mark) that some editors add\n    if config_str.startswith('\ufeff'):\n        config_str = config_str[1:]\n    \n    return json.loads(config_str)\n```\n\nThis handles:\n- Empty strings\n- Unicode BOM prefix\n- But you may also want to handle: malformed JSON, None input, non-string input",
    "context": {
      "target_categories": [
        "edge_cases"
      ],
      "difficulty": "medium",
      "rationale": "Shows edge cases like empty input and unicode BOM, while hinting at more",
      "source": "synthetic-discovery-generator",
      "generated_at": "2026-01-08T10:51:57.526559"
    }
  },
  {
    "input": "Is there a bug in this pagination function?\n```python\ndef get_page(items, page_num, page_size=10):\n    start = page_num * page_size\n    end = start + page_size\n    return items[start:end]\n```",
    "output": "Yes, there's an off-by-one issue if page_num is 1-indexed (as users expect):\n\n```python\ndef get_page(items, page_num, page_size=10):\n    # If page_num is 1-indexed (first page = 1)\n    start = (page_num - 1) * page_size\n    end = start + page_size\n    return items[start:end]\n```\n\nHowever, the original is correct if page_num is 0-indexed. You should:\n1. Document which convention you're using\n2. Validate page_num >= 1 (or >= 0)\n3. Consider what to return for page_num beyond the data (empty list is fine)",
    "context": {
      "target_categories": [
        "boundary_conditions"
      ],
      "difficulty": "easy",
      "rationale": "Classic off-by-one boundary condition with pagination indexing",
      "source": "synthetic-discovery-generator",
      "generated_at": "2026-01-08T10:51:57.526562"
    }
  },
  {
    "input": "Why does this SQL query sometimes return wrong results?\n```python\ndef get_user(db, username):\n    query = f\"SELECT * FROM users WHERE username = '{username}'\"\n    return db.execute(query).fetchone()\n```",
    "output": "This code has a **critical SQL injection vulnerability**! A malicious username like `' OR '1'='1` would return all users.\n\nSafe version using parameterized queries:\n\n```python\ndef get_user(db, username: str):\n    query = \"SELECT * FROM users WHERE username = ?\"\n    return db.execute(query, (username,)).fetchone()\n```\n\nThe issue is string interpolation in SQL. Always use:\n- Parameterized queries (shown above)\n- Or an ORM like SQLAlchemy that handles escaping",
    "context": {
      "target_categories": [
        "failure_modes"
      ],
      "difficulty": "medium",
      "rationale": "Security vulnerability (SQL injection) is a critical failure mode",
      "source": "synthetic-discovery-generator",
      "generated_at": "2026-01-08T10:51:57.526565"
    }
  },
  {
    "input": "Write a function to validate an email address.",
    "output": "Here's a basic email validator:\n\n```python\nimport re\n\ndef is_valid_email(email: str) -> bool:\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return bool(re.match(pattern, email))\n```\n\n**However, I need more context to give you the best solution:**\n- Should it check if the domain exists (DNS lookup)?\n- Should it handle international email addresses (IDN)?\n- Is this for user signup (send verification email) or data validation?\n- What's the performance requirement (regex vs library)?\n\nFor production, consider using a library like `email-validator` which handles edge cases properly.",
    "context": {
      "target_categories": [
        "missing_info"
      ],
      "difficulty": "medium",
      "rationale": "The request lacks context about validation depth, use case, and requirements",
      "source": "synthetic-discovery-generator",
      "generated_at": "2026-01-08T10:51:57.526568"
    }
  },
  {
    "input": "Should I use a class or functions for this data processing pipeline?\n```python\n# Current approach with functions:\ndef load_data(path): ...\ndef clean_data(df): ...\ndef transform_data(df): ...\ndef save_data(df, path): ...\n\n# Run pipeline\ndf = load_data(\"input.csv\")\ndf = clean_data(df)\ndf = transform_data(df)\nsave_data(df, \"output.csv\")\n```",
    "output": "Both approaches are valid! Here are the trade-offs:\n\n**Functional approach (current):**\n- \u2705 Simple and readable\n- \u2705 Easy to test individual functions\n- \u2705 Works well for scripts and notebooks\n- \u274c State must be passed explicitly\n\n**Class-based approach:**\n```python\nclass DataPipeline:\n    def __init__(self, input_path):\n        self.df = None\n        self.input_path = input_path\n    \n    def run(self, output_path):\n        self.load().clean().transform().save(output_path)\n        return self\n```\n- \u2705 Encapsulates state\n- \u2705 Method chaining is elegant\n- \u2705 Easier to extend with logging, caching\n- \u274c More boilerplate\n\n**My recommendation:** Start with functions. Refactor to a class when you need:\n- Multiple pipelines with different configs\n- Caching intermediate results\n- Complex error handling/retry logic",
    "context": {
      "target_categories": [
        "disagreements"
      ],
      "difficulty": "hard",
      "rationale": "Classic design debate with no single correct answer - depends on context",
      "source": "synthetic-discovery-generator",
      "generated_at": "2026-01-08T10:51:57.526576"
    }
  },
  {
    "input": "I have this TypeScript function used across our codebase. Can you refactor it to be \u201ccleaner\u201d and more maintainable without changing behavior?\n\n```ts\nexport function doThing(u:any, a:any, b:any, c?:any){\n  let r:any = {};\n  if(u && u.id){\n    r.uid = u.id\n  } else {\n    r.uid = null\n  }\n\n  // flags\n  r.f1 = a?true:false\n  r.f2 = b?true:false\n  r.f3 = c?true:false\n\n  // compute score\n  let s = 0\n  if(a){s += 10}\n  if(b){s += 20}\n  if(c){s += 30}\n  r.score = s\n\n  // metadata\n  r.meta = { at: Date.now(), by: u && u.name ? u.name : \"\" }\n\n  return r\n}\n```\n\nConstraints:\n- Keep the same output shape and truthiness behavior.\n- Don\u2019t add dependencies.\n- Make it easier to test.\n",
    "output": "Here\u2019s a refactor that keeps the same output shape and truthiness semantics, but improves readability, typing, and testability by isolating pure logic.\n\n```ts\ntype UserLike = { id?: unknown; name?: unknown } | null | undefined;\n\ntype Result = {\n  uid: unknown | null;\n  f1: boolean;\n  f2: boolean;\n  f3: boolean;\n  score: number;\n  meta: { at: number; by: string };\n};\n\n/**\n * Pure builder: inject `now` for testability.\n * Preserves original truthiness behavior (no boolean coercion changes beyond !!).\n */\nexport function buildResult(\n  u: UserLike,\n  a: unknown,\n  b: unknown,\n  c?: unknown,\n  now: () => number = Date.now\n): Result {\n  const uid = u && (u as any).id ? (u as any).id : null;\n\n  const f1 = !!a;\n  const f2 = !!b;\n  const f3 = !!c;\n\n  const score = (f1 ? 10 : 0) + (f2 ? 20 : 0) + (f3 ? 30 : 0);\n\n  const by = u && (u as any).name ? String((u as any).name) : \"\";\n\n  return {\n    uid,\n    f1,\n    f2,\n    f3,\n    score,\n    meta: { at: now(), by },\n  };\n}\n\n// Backward-compatible wrapper name/signature\nexport function doThing(u: any, a: any, b: any, c?: any) {\n  return buildResult(u, a, b, c);\n}\n```\n\nNotes on \u201cno behavior change\u201d:\n- `uid` remains `null` when `u` is falsy or `u.id` is falsy; otherwise it\u2019s the original `u.id` value (no casting).\n- Flags keep the same truthiness coercion (`a?true:false` == `!!a`).\n- `score` is derived from the same conditions as before.\n- `meta.by` stays `\"\"` unless `u.name` is truthy; it\u2019s stringified to avoid leaking non-string types into a field that\u2019s conceptually a string (if you truly need to preserve non-string `name` values, remove `String(...)`).\n\nTesting becomes simpler because you can inject time:\n\n```ts\nimport { buildResult } from \"./doThing\";\n\ntest(\"buildResult\", () => {\n  const out = buildResult({ id: 123, name: \"A\" }, 1, 0, \"x\", () => 42);\n  expect(out).toEqual({\n    uid: 123,\n    f1: true,\n    f2: false,\n    f3: true,\n    score: 40,\n    meta: { at: 42, by: \"A\" },\n  });\n});\n```\n\nIf you want, I can also align naming (`doThing` \u2192 something domain-specific) and add minimal JSDoc describing what the flags/score represent\u2014those are usually the main maintainability pain points here.",
    "context": {
      "target_categories": [
        "themes"
      ],
      "difficulty": "medium",
      "rationale": "Targets the \u201cthemes\u201d category by focusing on maintainability improvements: clearer naming, stronger typing without extra dependencies, separation of pure logic for testability, reduced repetition, and explicit documentation. It also introduces a small, realistic style/design choice (stringifying `name`) that invites discussion about code quality expectations and consistency across a codebase without being an edge-case/boundary-condition-focused prompt.",
      "source": "synthetic-discovery-generator",
      "generated_at": "2026-01-08T10:51:57.526585"
    }
  },
  {
    "input": "I'm doing a quick refactor pass on a small Python module that fetches JSON from a URL and writes it to disk. It works, but the code feels messy and hard to maintain. Can you refactor it for readability and maintainability (naming, structure, docstrings, type hints), without changing behavior?\n\n```python\nimport json, requests, os\n\ndef do(u, p, t=10):\n    r = requests.get(u, timeout=t)\n    if r.status_code != 200:\n        print('bad', r.status_code)\n        return False\n    d = r.json()\n    if not os.path.exists(os.path.dirname(p)):\n        os.makedirs(os.path.dirname(p))\n    f = open(p, 'w')\n    f.write(json.dumps(d))\n    f.close()\n    return True\n```\n\nConstraints:\n- Keep `requests`.\n- Keep the function callable as `do(url, path)` (can add new functions/classes though).\n- No new third-party dependencies.\n",
    "output": "Here\u2019s a refactor that keeps the same external behavior (returns `True` on HTTP 200 and successful write; prints and returns `False` on non-200), but improves readability, structure, and maintainability.\n\nKey changes:\n- Clearer names (`fetch_json`, `ensure_parent_dir`, `write_json_file`).\n- Type hints and docstrings.\n- Uses context managers for file handling.\n- Avoids repeated `os.path.dirname(p)` calls.\n- Keeps `do(url, path, t=10)` signature and semantics.\n\n```python\nfrom __future__ import annotations\n\nimport json\nimport os\nfrom typing import Any\n\nimport requests\n\n\ndef fetch_json(url: str, timeout_seconds: int = 10) -> tuple[int, Any]:\n    \"\"\"Fetch JSON from `url`.\n\n    Returns:\n        (status_code, parsed_json)\n\n    Notes:\n        This function does not raise on non-200 status codes; it simply returns\n        the status code and whatever `Response.json()` produces on success.\n    \"\"\"\n    response = requests.get(url, timeout=timeout_seconds)\n    return response.status_code, response.json()\n\n\ndef ensure_parent_dir(file_path: str) -> None:\n    \"\"\"Create the parent directory for `file_path` if it doesn't exist.\"\"\"\n    parent_dir = os.path.dirname(file_path)\n    if parent_dir and not os.path.exists(parent_dir):\n        os.makedirs(parent_dir)\n\n\ndef write_json_file(file_path: str, data: Any) -> None:\n    \"\"\"Write `data` as JSON to `file_path`.\"\"\"\n    # Preserve original behavior: default json.dumps settings, no indentation.\n    payload = json.dumps(data)\n    with open(file_path, \"w\") as fp:\n        fp.write(payload)\n\n\ndef do(u: str, p: str, t: int = 10) -> bool:\n    \"\"\"Fetch JSON from URL `u` and write it to path `p`.\n\n    Returns:\n        True if the HTTP status is 200 and the file is written.\n        False if the HTTP status is not 200 (prints \"bad <status>\").\n\n    Behavior intentionally mirrors the original implementation.\n    \"\"\"\n    status_code, data = fetch_json(u, timeout_seconds=t)\n\n    if status_code != 200:\n        print(\"bad\", status_code)\n        return False\n\n    ensure_parent_dir(p)\n    write_json_file(p, data)\n    return True\n```\n\nIf you want to keep *all* behavior identical, note that the original code will throw exceptions for things like invalid JSON, network errors, or permission issues; this refactor preserves that (i.e., it doesn\u2019t add try/except). If you\u2019d like a \u201csafe\u201d variant that returns `False` on those failures too, say so and I can adjust while keeping the same `do(url, path)` entry point.",
    "context": {
      "target_categories": [
        "themes"
      ],
      "difficulty": "medium",
      "rationale": "Targets the \u201cthemes\u201d category by focusing on maintainability and code quality: clearer naming, modular structure, docstrings, type hints, context managers, and reduced repetition\u2014without introducing edge-case handling or changing functional behavior. Also invites discussion about whether to preserve exception behavior vs. add robustness, a common maintainability theme.",
      "source": "synthetic-discovery-generator",
      "generated_at": "2026-01-08T10:51:57.526594"
    }
  },
  {
    "input": "I wrote this Python function to parse a CSV line into fields, but it breaks on some inputs. Can you fix it and show a few tests for tricky edge cases?\n\n```py\ndef parse_csv_line(line: str):\n    # supports quoted fields\n    fields = []\n    cur = \"\"\n    in_quotes = False\n    for ch in line:\n        if ch == '\"':\n            in_quotes = not in_quotes\n        elif ch == ',' and not in_quotes:\n            fields.append(cur)\n            cur = \"\"\n        else:\n            cur += ch\n    fields.append(cur)\n    return fields\n```\n\nExamples that should work:\n- `a,b,c`\n- `\"a,b\",c`\n\nExamples that currently fail:\n- `\"a\"\"b\",c` (escaped quote)\n- trailing comma like `a,b,`\n- empty line\n",
    "output": "Here\u2019s a small, robust fix that handles the common CSV edge cases you listed (escaped quotes `\"\"`, trailing comma, and empty line) while keeping the implementation simple.\n\nKey change: when inside quotes and we see a `\"`, treat `\"\"` as an escaped quote instead of toggling quote-mode.\n\n```py\ndef parse_csv_line(line: str):\n    fields = []\n    cur_chars = []\n    in_quotes = False\n    i = 0\n    n = len(line)\n\n    while i < n:\n        ch = line[i]\n\n        if ch == '\"':\n            if in_quotes:\n                # Escaped quote inside a quoted field: \"\"\n                if i + 1 < n and line[i + 1] == '\"':\n                    cur_chars.append('\"')\n                    i += 2\n                    continue\n                # Otherwise, end of quoted section\n                in_quotes = False\n            else:\n                # Start of quoted section\n                in_quotes = True\n        elif ch == ',' and not in_quotes:\n            fields.append(''.join(cur_chars))\n            cur_chars = []\n        else:\n            cur_chars.append(ch)\n\n        i += 1\n\n    fields.append(''.join(cur_chars))\n    return fields\n```\n\nA few focused tests (plain `assert`s):\n\n```py\ndef test_parse_csv_line():\n    # basic\n    assert parse_csv_line(\"a,b,c\") == [\"a\", \"b\", \"c\"]\n\n    # comma inside quotes\n    assert parse_csv_line('\"a,b\",c') == [\"a,b\", \"c\"]\n\n    # escaped quote inside quoted field\n    assert parse_csv_line('\"a\"\"b\",c') == ['a\"b', 'c']\n\n    # trailing comma => last field is empty\n    assert parse_csv_line(\"a,b,\") == [\"a\", \"b\", \"\"]\n\n    # empty line => one empty field (common CSV interpretation)\n    assert parse_csv_line(\"\") == [\"\"]\n\n    # only commas => multiple empty fields\n    assert parse_csv_line(\",,\") == [\"\", \"\", \"\"]\n\n    # quoted empty field\n    assert parse_csv_line('\"\",x') == [\"\", \"x\"]\n\n    # spaces are preserved (no trimming)\n    assert parse_csv_line(' \"a\" ,b') == [\" a \", \"b\"]\n\n\ntest_parse_csv_line()\n```\n\nNotes on edge behavior:\n- This does **not** validate malformed CSV (e.g., unmatched quotes). If you want, we can add an option to raise on `in_quotes == True` at end-of-line.\n- It preserves whitespace exactly; if you need trimming outside quotes, that\u2019s a separate rule to define.",
    "context": {
      "target_categories": [
        "edge_cases"
      ],
      "difficulty": "easy",
      "rationale": "Focuses on edge cases in CSV parsing: escaped quotes, trailing delimiters, empty input, all-empty fields, quoted empty field, and whitespace preservation. The fix is minimal and test-driven to surface tricky inputs.",
      "source": "synthetic-discovery-generator",
      "generated_at": "2026-01-08T10:51:57.526598"
    }
  },
  {
    "input": "I wrote a tiny JS helper to get the file extension, but some edge cases are failing. Can you fix it and add a few tests?\n\n```js\nfunction ext(name) {\n  return name.split('.').pop().toLowerCase();\n}\n\nmodule.exports = { ext };\n```\n\nFailing examples:\n- `\"README\"` should return `\"\"`\n- `\".gitignore\"` should return `\"\"`\n- `\"archive.tar.gz\"` should return `\"gz\"`\n- `\"photo.JPG\"` should return `\"jpg\"`\n- `\"trailingdot.\"` should return `\"\"`\n- `\"dir/file.txt\"` should return `\"txt\"`\n- `\"dir.with.dots/file\"` should return `\"\"`\n- `\"\"` should return `\"\"`\n",
    "output": "Here\u2019s a version that handles the listed edge cases by:\n- only looking at the last path segment (after `/` or `\\`)\n- treating \u201cno dot\u201d, \u201cleading dot only\u201d, and \u201ctrailing dot\u201d as no extension\n\n```js\nfunction ext(path) {\n  if (!path) return \"\";\n\n  // Handle both POSIX and Windows separators\n  const base = String(path).split(/[\\\\/]/).pop();\n  if (!base) return \"\";\n\n  const lastDot = base.lastIndexOf(\".\");\n\n  // No dot\n  if (lastDot === -1) return \"\";\n  // Dot is first char (e.g., .gitignore) => treat as no extension\n  if (lastDot === 0) return \"\";\n  // Dot is last char (e.g., trailingdot.) => treat as no extension\n  if (lastDot === base.length - 1) return \"\";\n\n  return base.slice(lastDot + 1).toLowerCase();\n}\n\nmodule.exports = { ext };\n```\n\nMinimal Jest tests:\n\n```js\nconst { ext } = require('./ext');\n\ndescribe('ext', () => {\n  test('no extension', () => {\n    expect(ext('README')).toBe('');\n    expect(ext('')).toBe('');\n    expect(ext('dir.with.dots/file')).toBe('');\n  });\n\n  test('dotfiles and trailing dot', () => {\n    expect(ext('.gitignore')).toBe('');\n    expect(ext('trailingdot.')).toBe('');\n  });\n\n  test('normal extensions', () => {\n    expect(ext('archive.tar.gz')).toBe('gz');\n    expect(ext('photo.JPG')).toBe('jpg');\n    expect(ext('dir/file.txt')).toBe('txt');\n  });\n});\n```",
    "context": {
      "target_categories": [
        "edge_cases"
      ],
      "difficulty": "easy",
      "rationale": "Targets edge cases in string parsing: filenames without dots, dotfiles, trailing dots, multiple dots, path segments containing dots, empty input, and case normalization. The fix is small but explicitly addresses each tricky input and adds tests that mirror the failing examples.",
      "source": "synthetic-discovery-generator",
      "generated_at": "2026-01-08T10:51:57.526604"
    }
  },
  {
    "input": "I wrote a Python function to split a time range into fixed-size buckets. It mostly works but sometimes I get an extra bucket at the end or I miss the last bucket when the end time lands exactly on a boundary. Can you fix it?\n\nRules:\n- Input datetimes are timezone-aware.\n- Buckets are half-open intervals: [start, end)\n- Should return a list of (bucket_start, bucket_end) tuples.\n- If start == end, return [].\n- If start > end, raise ValueError.\n\nHere is my code:\n\n```py\nfrom datetime import datetime, timedelta\n\ndef bucketize(start: datetime, end: datetime, size: timedelta):\n    if size.total_seconds() <= 0:\n        raise ValueError(\"size must be positive\")\n    if start > end:\n        raise ValueError(\"start must be <= end\")\n\n    out = []\n    cur = start\n    while cur <= end:\n        nxt = cur + size\n        out.append((cur, min(nxt, end)))\n        cur = nxt\n    return out\n```\n\nExample that fails my expectations:\n- start=2025-01-01T00:00:00+00:00\n- end=2025-01-01T01:00:00+00:00\n- size=15 minutes\nI expect 4 buckets, but I get 5 with a last empty/zero bucket.\n\nAlso, if end is 00:59:59 I sometimes miss the last partial bucket.\n\nPlease fix and add a few tests that pin down the boundary behavior.",
    "output": "Your loop condition is the boundary bug: for half-open buckets [a,b), you should keep emitting buckets while `cur < end`, not `cur <= end`. When `cur == end`, there is no time left to cover, so emitting a bucket creates a zero-length interval.\n\nAlso, to ensure you never \u201cmiss\u201d the last partial bucket, you should always append one when `cur < end` even if `cur + size > end` (by clamping the bucket end to `end`). Your existing `min(nxt, end)` already does that; the `<=` is what causes the extra empty bucket.\n\nFixed implementation:\n\n```py\nfrom __future__ import annotations\n\nfrom datetime import datetime, timedelta\n\n\ndef bucketize(start: datetime, end: datetime, size: timedelta):\n    if size.total_seconds() <= 0:\n        raise ValueError(\"size must be positive\")\n    if start > end:\n        raise ValueError(\"start must be <= end\")\n    if start == end:\n        return []\n\n    out = []\n    cur = start\n    while cur < end:  # half-open: stop when cur reaches end\n        nxt = cur + size\n        bucket_end = end if nxt > end else nxt\n        out.append((cur, bucket_end))\n        cur = nxt\n    return out\n```\n\nPytest tests that pin down the boundary conditions:\n\n```py\nimport pytest\nfrom datetime import datetime, timedelta, timezone\n\nfrom your_module import bucketize\n\n\ndef dt(s: str) -> datetime:\n    # e.g. \"2025-01-01T00:00:00+00:00\"\n    return datetime.fromisoformat(s)\n\n\ndef test_exact_end_on_boundary_no_extra_bucket():\n    start = dt(\"2025-01-01T00:00:00+00:00\")\n    end = dt(\"2025-01-01T01:00:00+00:00\")\n    size = timedelta(minutes=15)\n\n    buckets = bucketize(start, end, size)\n\n    assert len(buckets) == 4\n    assert buckets[0] == (start, dt(\"2025-01-01T00:15:00+00:00\"))\n    assert buckets[-1] == (dt(\"2025-01-01T00:45:00+00:00\"), end)\n\n\ndef test_end_not_on_boundary_includes_last_partial_bucket():\n    start = dt(\"2025-01-01T00:00:00+00:00\")\n    end = dt(\"2025-01-01T00:59:59+00:00\")\n    size = timedelta(minutes=15)\n\n    buckets = bucketize(start, end, size)\n\n    assert len(buckets) == 4\n    assert buckets[-1] == (dt(\"2025-01-01T00:45:00+00:00\"), end)\n\n\ndef test_start_equals_end_returns_empty():\n    t = dt(\"2025-01-01T00:00:00+00:00\")\n    assert bucketize(t, t, timedelta(minutes=15)) == []\n\n\ndef test_start_greater_than_end_raises():\n    start = dt(\"2025-01-01T01:00:00+00:00\")\n    end = dt(\"2025-01-01T00:00:00+00:00\")\n    with pytest.raises(ValueError):\n        bucketize(start, end, timedelta(minutes=15))\n\n\ndef test_size_must_be_positive():\n    start = dt(\"2025-01-01T00:00:00+00:00\")\n    end = dt(\"2025-01-01T01:00:00+00:00\")\n    with pytest.raises(ValueError):\n        bucketize(start, end, timedelta(0))\n```\n\nThese tests specifically cover the tricky boundary where `end` is exactly on a bucket boundary (no extra zero-length bucket) and where `end` is not on a boundary (last partial bucket is included).",
    "context": {
      "target_categories": [
        "boundary_conditions"
      ],
      "difficulty": "hard",
      "rationale": "Targets boundary_conditions by focusing on inclusive vs exclusive loop termination for half-open intervals, the start==end empty-range boundary, and end exactly on a bucket boundary producing a zero-length bucket. The tests explicitly lock down off-by-one behavior at boundaries without drifting into other categories.",
      "source": "synthetic-discovery-generator",
      "generated_at": "2026-01-08T10:51:57.526611"
    }
  },
  {
    "input": "I'm getting an off-by-one bug in this Python function that checks if a timestamp is within business hours. It should return True for times between 09:00 and 17:00 inclusive on both ends, but I'm seeing it return False at exactly 17:00.\n\n```python\nfrom datetime import time\n\ndef in_business_hours(t: time) -> bool:\n    start = time(9, 0)\n    end = time(17, 0)\n    return start <= t < end\n\nprint(in_business_hours(time(9,0)))   # expected True\nprint(in_business_hours(time(16,59))) # expected True\nprint(in_business_hours(time(17,0)))  # expected True, but False\n```\n\nCan you fix it and add a couple tests around the boundaries so I don't regress this?",
    "output": "You\u2019re using a half-open interval (`start <= t < end`), which excludes `end`. Since you want *inclusive* on both ends, switch the upper comparison to `<=`.\n\n### Fix\n```python\nfrom datetime import time\n\ndef in_business_hours(t: time) -> bool:\n    start = time(9, 0)\n    end = time(17, 0)\n    return start <= t <= end\n```\n\n### Boundary-focused tests (pytest)\n```python\nfrom datetime import time\n\nfrom your_module import in_business_hours\n\n\ndef test_in_business_hours_inclusive_start():\n    assert in_business_hours(time(9, 0)) is True\n\n\ndef test_in_business_hours_just_before_end():\n    assert in_business_hours(time(16, 59)) is True\n\n\ndef test_in_business_hours_inclusive_end():\n    assert in_business_hours(time(17, 0)) is True\n\n\ndef test_in_business_hours_just_after_end():\n    assert in_business_hours(time(17, 1)) is False\n```\n\nThese lock down the exact boundary conditions (start, just before end, end, and just after end) so an accidental reintroduction of a half-open interval will be caught.",
    "context": {
      "target_categories": [
        "boundary_conditions"
      ],
      "difficulty": "easy",
      "rationale": "Targets boundary_conditions via an explicit inclusive vs exclusive comparison bug at the upper bound (17:00). The request and solution focus narrowly on off-by-one behavior at exact endpoints and tests that pin down behavior immediately around the boundaries.",
      "source": "synthetic-discovery-generator",
      "generated_at": "2026-01-08T10:51:57.526619"
    }
  },
  {
    "input": "I have this Python function that sends events to a webhook. In production, sometimes events just disappear (no retries?) and occasionally the process CPU spikes. Can you spot failure modes and suggest a fix?\n\n```python\nimport json, threading, requests\n\nqueue = []\n\n\ndef send_event(url, payload):\n    queue.append(payload)\n\n    def worker():\n        while queue:\n            item = queue.pop(0)\n            requests.post(url, data=json.dumps(item), headers={\"Content-Type\": \"application/json\"})\n\n    threading.Thread(target=worker).start()\n```\n\nConstraints: keep it simple (no Celery/Kafka).",
    "output": "Main failure modes in the current code\n\n1) **Data race / lost events**\n- `queue` is a shared global list accessed from multiple threads with no lock.\n- Two workers can interleave `while queue:` and `pop(0)` causing inconsistent behavior (including `IndexError`), and events can be dropped if a worker crashes.\n\n2) **Unbounded thread creation (CPU spikes + resource exhaustion)**\n- Every `send_event()` starts a new thread. Under load you can create hundreds/thousands of threads contending on the GIL and network, causing CPU spikes and memory pressure.\n\n3) **No error handling / no timeouts => silent loss + hangs**\n- `requests.post(...)` has no `timeout`. A stuck TCP connection can block a worker thread indefinitely.\n- Exceptions from `requests` (DNS failure, connection error, non-2xx if you call `raise_for_status`) are not handled; the thread will terminate and any remaining queued items are never sent.\n\n4) **Inefficient queue implementation**\n- `pop(0)` is O(n) because it shifts the entire list each time; under load this can contribute to CPU spikes.\n\nA simple, robust fix (single background worker + thread-safe queue + retries)\n\n```python\nimport json\nimport threading\nimport time\nfrom queue import Queue, Empty\n\nimport requests\n\n\nclass WebhookSender:\n    def __init__(self, url: str, *, max_queue: int = 10_000):\n        self.url = url\n        self.q: Queue[dict] = Queue(maxsize=max_queue)\n        self._stop = threading.Event()\n        self._thread = threading.Thread(target=self._worker, name=\"webhook-sender\", daemon=True)\n        self._thread.start()\n\n    def send_event(self, payload: dict) -> None:\n        # If you prefer not to drop, use put() (blocks). If you prefer to drop, use put_nowait.\n        self.q.put(payload)\n\n    def close(self, *, drain: bool = True, timeout: float | None = None) -> None:\n        if not drain:\n            self._stop.set()\n        else:\n            # signal stop after draining\n            self._stop.set()\n        self._thread.join(timeout=timeout)\n\n    def _post_with_retries(self, payload: dict, *, attempts: int = 3) -> None:\n        body = json.dumps(payload)\n        for i in range(attempts):\n            try:\n                resp = requests.post(\n                    self.url,\n                    data=body,\n                    headers={\"Content-Type\": \"application/json\"},\n                    timeout=(3.05, 10),  # connect, read\n                )\n                # Treat non-2xx as failure so we retry\n                if 200 <= resp.status_code < 300:\n                    return\n                # Optional: respect 429/503 with backoff\n                raise RuntimeError(f\"HTTP {resp.status_code}: {resp.text[:200]}\")\n            except Exception:\n                # exponential backoff with cap\n                if i == attempts - 1:\n                    # At this point, decide: requeue, log+drop, or persist to disk.\n                    # Keeping it simple: drop after last attempt.\n                    return\n                time.sleep(min(2 ** i, 8))\n\n    def _worker(self) -> None:\n        while not self._stop.is_set() or not self.q.empty():\n            try:\n                payload = self.q.get(timeout=0.2)\n            except Empty:\n                continue\n            try:\n                self._post_with_retries(payload)\n            finally:\n                self.q.task_done()\n```\n\nWhy this addresses the failures\n- Uses `queue.Queue`, which is thread-safe, eliminating races and list corruption.\n- Uses **one** long-lived daemon thread instead of spawning a thread per event.\n- Adds **timeouts** so network hangs don\u2019t stall forever.\n- Adds **basic retry** logic so transient failures don\u2019t silently drop events.\n\nIf you can answer one question, it affects the \u201cfailure mode\u201d strategy: when the webhook is down for a long time, do you want to (a) block producers, (b) drop events, or (c) persist to disk for later replay? The code above currently blocks when the queue is full and drops after final retry, but it\u2019s easy to change based on your preference.",
    "context": {
      "target_categories": [
        "failure_modes"
      ],
      "difficulty": "medium",
      "rationale": "Targets failure_modes by presenting a realistic webhook sender with multiple production-impacting failures: unsynchronized shared state leading to lost events/crashes, unbounded thread creation causing CPU spikes, lack of timeouts causing hangs, lack of exception handling causing silent drops, and inefficient pop(0) overhead. The proposed fix directly mitigates these failure modes with a thread-safe queue, single worker, timeouts, and retries while staying within constraints (no heavy infra).",
      "source": "synthetic-discovery-generator",
      "generated_at": "2026-01-08T10:51:57.526630"
    }
  },
  {
    "input": "I wrote this Python function to read a JSON config file. It works on my machine, but in production sometimes the service crashes or returns weird errors. Can you spot the failure modes and suggest a safer version?\n\n```py\nimport json\n\nCONFIG = {}\n\ndef load_config(path):\n    global CONFIG\n    f = open(path)\n    CONFIG = json.loads(f.read())\n    return CONFIG\n\n\ndef get(key, default=None):\n    return CONFIG[key] if key in CONFIG else default\n```\n",
    "output": "Main failure modes in the current code:\n\n1. **File descriptor leak**: `open(path)` is never closed. Under load (or repeated reloads), you can hit \u201ctoo many open files\u201d.\n2. **Crashes on bad/missing input**:\n   - Missing file \u2192 `FileNotFoundError`.\n   - Invalid JSON \u2192 `json.JSONDecodeError`.\n   - Permission issues \u2192 `PermissionError`.\n   These can crash the service if uncaught.\n3. **Partial/unsafe global update**: If an exception happens after `open()` (or during parsing), `CONFIG` may remain stale, and callers may see inconsistent behavior. Also, multiple threads calling `load_config()` / `get()` can race.\n4. **Unhelpful error context**: When it fails, you don\u2019t include which file/path caused it.\n\nA safer version (minimal changes, better failure behavior):\n\n```py\nimport json\nfrom typing import Any, Dict\n\nCONFIG: Dict[str, Any] = {}\n\n\ndef load_config(path: str) -> Dict[str, Any]:\n    \"\"\"Load config from JSON file.\n\n    Failure behavior:\n    - Raises with clear context on file/JSON errors.\n    - Only updates CONFIG after successful parse.\n    \"\"\"\n    global CONFIG\n\n    try:\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)  # avoids reading whole file into memory\n    except (OSError, json.JSONDecodeError) as e:\n        # Keep existing CONFIG unchanged; re-raise with context\n        raise RuntimeError(f\"Failed to load config from {path!r}: {e}\") from e\n\n    if not isinstance(data, dict):\n        raise RuntimeError(f\"Config in {path!r} must be a JSON object (dict), got {type(data).__name__}\")\n\n    CONFIG = data\n    return CONFIG\n\n\ndef get(key: str, default: Any = None) -> Any:\n    return CONFIG.get(key, default)\n```\n\nIf you also have concurrency in production, consider adding a lock around updates/reads (or returning an immutable snapshot) to avoid race-related failures.",
    "context": {
      "target_categories": [
        "failure_modes"
      ],
      "difficulty": "easy",
      "rationale": "This trace targets failure_modes by presenting a small, realistic config loader with production-impacting faults: leaked file handles, unhandled exceptions (missing file/invalid JSON/permissions), unsafe global state updates, and poor error context. The response focuses on identifying and mitigating these failure modes with context managers, atomic update after successful parse, explicit type validation, and clearer exception wrapping; it briefly notes concurrency as another failure mode without shifting into broader design debates.",
      "source": "synthetic-discovery-generator",
      "generated_at": "2026-01-08T10:51:57.526634"
    }
  },
  {
    "input": "I need a function to \u201csanitize\u201d usernames before saving them. Can you write it in Python?",
    "output": "A good implementation depends on a few requirements that aren\u2019t specified yet. Before I code it, I need these details (otherwise I might \u201csanitize\u201d in a way that breaks your UX or security model):\n\n1) **Allowed character set**\n   - Do you want only `[a-z0-9_]`? Are hyphens/dots allowed?\n   - Should we allow non-ASCII (e.g., `Jos\u00e9`, `\u5f20\u4f1f`) or force ASCII?\n\n2) **Normalization rules**\n   - Should it be case-insensitive? (e.g., store lowercase always)\n   - Should Unicode be normalized (NFC/NFKC) to avoid lookalikes / duplicates?\n\n3) **Length constraints**\n   - Min/max length? What to do if it becomes empty after sanitizing?\n   - If too long: truncate, error, or hash?\n\n4) **Whitespace and separators**\n   - Replace spaces with underscore, collapse multiple separators, trim leading/trailing `_`/`-`?\n\n5) **Uniqueness policy**\n   - If two different inputs sanitize to the same username, do you want to reject the second, or auto-suffix (`name`, `name2`, ...)?\n\n6) **Reserved words / blocked patterns**\n   - Should we block `admin`, `root`, `support`, etc.?\n   - Should we forbid leading digits, consecutive dots, etc.?\n\n7) **Error handling vs \u201cbest effort\u201d**\n   - Should the function return a sanitized string no matter what, or raise/return an error when input can\u2019t be made valid?\n\n---\n\nWhile you confirm, here\u2019s a *template* that makes the policy explicit via parameters (so we can lock in behavior once you answer):\n\n```python\nimport re\nimport unicodedata\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass(frozen=True)\nclass UsernamePolicy:\n    allow_unicode: bool = False\n    lowercase: bool = True\n    min_len: int = 3\n    max_len: int = 20\n    allowed_re: str = r\"[a-z0-9_]+\"  # used when allow_unicode=False\n    replace_spaces_with: str = \"_\"\n    collapse_separators: bool = True\n    strip_separators: bool = True\n    reserved: frozenset[str] = frozenset({\"admin\", \"root\", \"support\"})\n\n\ndef sanitize_username(raw: str, policy: UsernamePolicy = UsernamePolicy()) -> str:\n    if raw is None:\n        raise TypeError(\"username cannot be None\")\n\n    s = raw.strip()\n\n    # Unicode normalization: NFKC helps reduce visually-confusable variants.\n    s = unicodedata.normalize(\"NFKC\", s)\n\n    if policy.lowercase:\n        s = s.lower()\n\n    if not policy.allow_unicode:\n        # Replace whitespace\n        s = re.sub(r\"\\s+\", policy.replace_spaces_with, s)\n        # Remove disallowed chars\n        s = re.sub(r\"[^a-z0-9_]\", \"\", s)\n    else:\n        # Minimal example for unicode-allowed: still normalize whitespace\n        s = re.sub(r\"\\s+\", policy.replace_spaces_with, s)\n        # You likely still want a unicode-aware allowlist here.\n\n    if policy.collapse_separators:\n        s = re.sub(r\"_+\", \"_\", s)\n\n    if policy.strip_separators:\n        s = s.strip(\"_\")\n\n    if len(s) < policy.min_len:\n        raise ValueError(f\"username too short after sanitizing: {s!r}\")\n    if len(s) > policy.max_len:\n        s = s[: policy.max_len]\n\n    if s in policy.reserved:\n        raise ValueError(\"username is reserved\")\n\n    return s\n```\n\nIf you answer the 7 questions (even briefly), I can finalize this into the exact behavior you want (including whether to raise vs return a fallback, and how to handle collisions).",
    "context": {
      "target_categories": [
        "missing_info"
      ],
      "difficulty": "medium",
      "rationale": "The user request is intentionally underspecified (\u201csanitize usernames\u201d) and the assistant response focuses on eliciting missing requirements (allowed chars, unicode/case normalization, length, uniqueness, reserved words, error strategy). It provides a parameterized template but highlights that correct behavior depends on clarifying information, directly targeting the missing_info category.",
      "source": "synthetic-discovery-generator",
      "generated_at": "2026-01-08T10:51:57.526644"
    }
  },
  {
    "input": "I need a function to \u201cnormalize\u201d phone numbers in our app. It should take whatever the user types and return a consistent format. Can you write it in Python?",
    "output": "Before writing code, I need a few specifics because \u201cnormalize phone numbers\u201d can mean different (and incompatible) things depending on product/region:\n\n1) **Target format**: Do you want **E.164** (e.g., `+14155552671`), a national display format (e.g., `(415) 555-2671`), or both (store E.164 + show pretty)?\n2) **Default region**: If a user enters `4155552671` with no country code, what country should we assume (US? based on user profile? based on locale)?\n3) **Extensions**: Should we support `x123`, `ext 123`, `#123`? If yes, how should extensions be represented in the output (drop, separate field, `;ext=` style)?\n4) **Allowed input characters**: Should we accept letters in vanity numbers (`1-800-FLOWERS`), full-width digits, Arabic-Indic digits, etc.?\n5) **Validation vs best-effort**: If the input is ambiguous/invalid, should the function raise, return `None`, return the original string, or return a structured error?\n6) **Storage vs display**: Are you normalizing for **deduplication** (strict) or **UI display** (lenient)?\n7) **Dependencies**: Is it acceptable to use `phonenumbers` (Google\u2019s libphonenumber port), which is the most reliable way, or must it be stdlib-only?\n\n---\n\n### If you can use `phonenumbers` (recommended)\nThis gives correct international parsing/validation.\n\n```python\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nimport phonenumbers\nfrom phonenumbers import NumberParseException\n\n\n@dataclass(frozen=True)\nclass NormalizedPhone:\n    e164: str\n    country_code: int\n    national: str\n    extension: Optional[str] = None\n\n\ndef normalize_phone(raw: str, default_region: str = \"US\") -> Optional[NormalizedPhone]:\n    \"\"\"Parse and normalize a phone number.\n\n    Returns NormalizedPhone if parseable and valid; otherwise None.\n    \"\"\"\n    if raw is None:\n        return None\n\n    s = raw.strip()\n    if not s:\n        return None\n\n    try:\n        num = phonenumbers.parse(s, default_region)\n    except NumberParseException:\n        return None\n\n    if not phonenumbers.is_possible_number(num) or not phonenumbers.is_valid_number(num):\n        return None\n\n    e164 = phonenumbers.format_number(num, phonenumbers.PhoneNumberFormat.E164)\n    national = phonenumbers.format_number(num, phonenumbers.PhoneNumberFormat.NATIONAL)\n\n    ext = num.extension if num.extension else None\n\n    return NormalizedPhone(\n        e164=e164,\n        country_code=num.country_code,\n        national=national,\n        extension=ext,\n    )\n```\n\nUsage:\n```python\nnormalize_phone(\"(415) 555-2671\", default_region=\"US\")\n# -> NormalizedPhone(e164='+14155552671', ...)\n\nnormalize_phone(\"+44 20 7946 0958\")\n# -> '+442079460958'\n\nnormalize_phone(\"+1 415 555 2671 ext 9\")\n# -> extension='9'\n```\n\n---\n\n### If you must be stdlib-only (best-effort, region-limited)\nThis is only safe if your app is effectively single-country and you accept imperfect parsing.\n\n```python\nimport re\nfrom typing import Optional\n\n_EXT_RE = re.compile(r\"(?:ext\\.?|x)\\s*(\\d+)$\", re.IGNORECASE)\n\n\ndef normalize_phone_us_best_effort(raw: str) -> Optional[str]:\n    \"\"\"Best-effort US-only normalization to E.164.\n\n    Returns '+1XXXXXXXXXX' or None.\n    \"\"\"\n    if raw is None:\n        return None\n\n    s = raw.strip()\n    if not s:\n        return None\n\n    # Strip extension (or choose to return it separately)\n    s = _EXT_RE.sub(\"\", s).strip()\n\n    # Keep digits only\n    digits = re.sub(r\"\\D\", \"\", s)\n\n    # Handle leading 1\n    if len(digits) == 11 and digits.startswith(\"1\"):\n        digits = digits[1:]\n\n    if len(digits) != 10:\n        return None\n\n    return \"+1\" + digits\n```\n\n---\n\nIf you answer (1) target format, (2) default region strategy, (3) extension handling, and (7) whether dependencies are allowed, I can tailor the final function and tests to your exact rules.",
    "context": {
      "target_categories": [
        "missing_info"
      ],
      "difficulty": "medium",
      "rationale": "The user request is intentionally underspecified (\u201cnormalize phone numbers\u201d) and can\u2019t be correctly implemented without clarifying requirements (format, region assumptions, validation behavior, extension handling, dependency constraints). The assistant response is designed to elicit that missing information and provides conditional templates rather than asserting a single correct behavior.",
      "source": "synthetic-discovery-generator",
      "generated_at": "2026-01-08T10:51:57.526652"
    }
  },
  {
    "input": "I\u2019m implementing a cache key function for API requests and my team is arguing about what should be part of the key. We use Python.\n\nRequirements (maybe):\n- Requests have: method, path, query params (dict of str->str), headers (dict), body (bytes or JSON), and a user_id.\n- We want high cache hit rate but must not leak data across users.\n\nDisagreement:\nA) Key = method + path + sorted query + body hash. Ignore headers and user_id.\nB) Key = method + path + sorted query + body hash + user_id.\nC) Key = method + full URL + normalized headers + body hash + user_id.\n\nCan you propose a `make_cache_key(req)` implementation and explain which approach you\u2019d pick? Also mention pitfalls so I can take it back to the team.\n\nHere\u2019s a skeleton:\n```py\nfrom dataclasses import dataclass\nfrom typing import Any\nimport hashlib\n\n@dataclass(frozen=True)\nclass Request:\n    method: str\n    path: str\n    query: dict[str, str]\n    headers: dict[str, str]\n    body: Any  # bytes | dict | None\n    user_id: str | None\n\n\ndef make_cache_key(req: Request) -> str:\n    ...\n```\n",
    "output": "Below is a pragmatic implementation plus a way to frame the disagreement: there isn\u2019t one universally \u201ccorrect\u201d key; it depends on whether the response varies by **user** and/or by **request headers** (auth, locale, content-negotiation, etc.).\n\n### Recommendation (default)\nPick **B** as the safe default for \u201cmust not leak data across users\u201d: include a **user scoping component** whenever the response could differ per user. Add **only the specific headers that actually affect the response** (rather than \u201call headers\u201d), which is a middle ground between B and C.\n\nIf you truly have endpoints that are public and identical for all users, you can opt them into a shared cache key (A) explicitly.\n\n### Implementation\nThis version:\n- Canonicalizes method/path\n- Canonicalizes query params deterministically\n- Hashes the body in a stable way (bytes vs JSON-like dict)\n- Scopes by user_id (or an explicit anonymous marker)\n- Includes a configurable allowlist of \u201cvary headers\u201d\n\n```py\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Any, Iterable\nimport hashlib\nimport json\n\n\n@dataclass(frozen=True)\nclass Request:\n    method: str\n    path: str\n    query: dict[str, str]\n    headers: dict[str, str]\n    body: Any  # bytes | dict | None\n    user_id: str | None\n\n\n# Only include headers known to affect representation.\n# Add/remove based on your API behavior.\nDEFAULT_VARY_HEADERS = (\n    \"accept\",\n    \"accept-language\",\n    \"content-type\",\n    # If you do conditional responses:\n    \"if-none-match\",\n    \"if-modified-since\",\n)\n\n\ndef _canonical_query(query: dict[str, str]) -> str:\n    # Sort by key then value; join with separators that won\u2019t collide easily.\n    # If you can have repeated keys, you\u2019ll want query as list[tuple[str,str]] instead.\n    items = sorted(query.items())\n    return \"&\".join(f\"{k}={v}\" for k, v in items)\n\n\ndef _canonical_headers(headers: dict[str, str], vary: Iterable[str]) -> str:\n    vary_lc = [h.lower() for h in vary]\n    picked = []\n    for name in vary_lc:\n        if name in headers:\n            picked.append((name, headers[name].strip()))\n    picked.sort()\n    return \"\\n\".join(f\"{k}:{v}\" for k, v in picked)\n\n\ndef _body_digest(body: Any) -> str:\n    h = hashlib.sha256()\n    if body is None:\n        h.update(b\"<empty>\")\n    elif isinstance(body, (bytes, bytearray, memoryview)):\n        h.update(bytes(body))\n    elif isinstance(body, str):\n        # If you sometimes pass JSON as a string, treat it as bytes of that string.\n        h.update(body.encode(\"utf-8\"))\n    else:\n        # Assume JSON-like object; canonicalize with stable key ordering.\n        # This is a design choice: if body can be arbitrary objects, you need an explicit serializer.\n        data = json.dumps(body, sort_keys=True, separators=(\",\", \":\"), ensure_ascii=False)\n        h.update(data.encode(\"utf-8\"))\n    return h.hexdigest()\n\n\ndef make_cache_key(\n    req: Request,\n    *,\n    vary_headers: Iterable[str] = DEFAULT_VARY_HEADERS,\n    scope_by_user: bool = True,\n) -> str:\n    method = req.method.upper().strip()\n    path = req.path  # assume already normalized by router; otherwise normalize here\n\n    q = _canonical_query(req.query or {})\n    hdr = _canonical_headers(req.headers or {}, vary_headers)\n    body = _body_digest(req.body)\n\n    if scope_by_user:\n        # If user_id is None, we still separate \u201canonymous\u201d from authenticated.\n        user_scope = req.user_id if req.user_id is not None else \"<anon>\"\n    else:\n        user_scope = \"<shared>\"\n\n    # Version prefix lets you rotate the scheme without invalidating code assumptions.\n    raw = \"\\n\".join([\n        \"v1\",\n        f\"m:{method}\",\n        f\"p:{path}\",\n        f\"q:{q}\",\n        f\"h:{hdr}\",\n        f\"b:{body}\",\n        f\"u:{user_scope}\",\n    ])\n\n    # Final key is a compact digest (better for memcached/redis key limits).\n    return \"ck:\" + hashlib.sha256(raw.encode(\"utf-8\")).hexdigest()\n```\n\n### How this maps to A/B/C (and how to explain it)\n- **A (ignore user_id + headers)**: maximizes hit rate but is only safe if responses are *guaranteed identical across users* and *don\u2019t vary by headers*. In practice, it\u2019s easy to be wrong (auth, AB tests, localization, content negotiation, feature flags). This is the \u201cfastest cache, riskiest leak\u201d option.\n- **B (include user_id)**: safe against cross-user leakage *if user_id aligns with authorization scope*. Still risky if the response varies by headers (e.g., `Accept-Language`) or other implicit context.\n- **C (include normalized headers + user_id)**: safest, but can destroy hit rate if you include too many headers (e.g., `User-Agent`, `Date`, tracing headers). The compromise is: include **only** headers that your server actually varies on.\n\n### Pitfalls to bring back to the team\n1. **Auth scope mismatch**: `user_id` may not reflect access rights (service accounts, org-level permissions, roles). Sometimes the right scope is `tenant_id`, `org_id`, or a hash of the auth principal/claims.\n2. **Header explosion**: including all headers is a common mistake; it tanks hit rate due to noisy headers (`User-Agent`, `X-Request-Id`, `Traceparent`, etc.). Prefer an allowlist.\n3. **Query representation**: if your framework allows repeated keys (`a=1&a=2`) or order-dependent params, a `dict` loses information. Consider representing query as `list[tuple[str,str]]` if that\u2019s possible.\n4. **Body canonicalization**: JSON objects are order-insensitive, but arrays are not. Also watch out for floats/decimals normalization.\n5. **Path normalization**: `/items/1` vs `/items/1/` or URL-decoding differences can fragment the cache.\n6. **Vary semantics**: HTTP has a concept for this (`Vary` header). If your upstream emits `Vary: Accept-Language`, your cache key should incorporate that header.\n\nIf you tell me whether your endpoints are mostly public vs per-user, and which headers you know affect responses (language, accept, feature flags), I can suggest a concrete `vary_headers` list and whether `scope_by_user` should be the default or opt-in.",
    "context": {
      "target_categories": [
        "disagreements"
      ],
      "difficulty": "medium",
      "rationale": "Designed to surface \u201cdisagreements\u201d where multiple answers are valid: choosing cache key components trades off hit rate vs correctness/security. The response explicitly frames A/B/C as context-dependent, proposes a compromise (user scoping + allowlisted vary headers), and highlights decision points (auth scope, Vary semantics, header noise) that teams commonly debate rather than a single definitive solution.",
      "source": "synthetic-discovery-generator",
      "generated_at": "2026-01-08T10:51:57.526659"
    }
  },
  {
    "input": "I need help deciding between REST and GraphQL for a new internal service. Context:\n- We have a web app + mobile app.\n- Data model: Projects -> Tasks -> Comments. Clients often need nested data.\n- We expect ~50k daily active users.\n- We have 6 backend engineers, 4 frontend.\n- Current stack: Node (NestJS) + Postgres.\n- We care about performance and developer velocity.\n\nI\u2019m not asking for a generic pros/cons list. I want a recommendation and a concrete plan (API shape, caching strategy, versioning, and how to avoid the usual pitfalls).",
    "output": "This is a \u201creasonable people can disagree\u201d choice; the right answer depends on how much your clients\u2019 data needs diverge over time and how much operational complexity you\u2019re willing to absorb.\n\n## Recommendation\n**Default to REST (or \u201cREST-ish JSON over HTTP\u201d) for this internal service, with a deliberate escape hatch**, unless you already know you\u2019ll have rapidly changing, highly client-specific query shapes.\n\nWhy REST-first fits your constraints:\n- **Team size/skill mix**: GraphQL\u2019s biggest costs are schema governance, resolver performance (N+1), auth at field level, and caching complexity. With 6 backend engineers, you can absolutely do it\u2014but it\u2019s an ongoing platform commitment.\n- **Your domain is naturally resource-oriented** (projects/tasks/comments) and maps cleanly to endpoints.\n- **Performance predictability**: REST makes it easier to reason about query plans, response sizes, and caching/CDN behavior.\n- **Mobile/web needs nested data**: You can cover this with a small number of \u201cexpanded\u201d endpoints without going full GraphQL.\n\nWhen I\u2019d flip to GraphQL:\n- You expect **frequent UI iteration** with lots of bespoke combinations of nested fields.\n- You want a **single typed contract** across web/mobile with strong tooling.\n- You can invest in DataLoader-style batching, persisted queries, and schema review discipline.\n\n## Concrete plan (REST-first with expansion)\n### 1) API shape\nUse stable resource endpoints plus controlled expansion. Example:\n\n- `GET /projects?limit=20&cursor=...`\n- `GET /projects/{projectId}`\n- `GET /projects/{projectId}/tasks?status=open&limit=...`\n- `GET /tasks/{taskId}`\n- `GET /tasks/{taskId}/comments?limit=...`\n\nFor nested fetches, provide:\n- `GET /projects/{projectId}?include=tasks`\n- `GET /projects/{projectId}?include=tasks,tasks.comments&tasksLimit=20&commentsLimit=5`\n\nGuardrails (important):\n- **Allowlist include paths** (don\u2019t let clients request arbitrary joins).\n- Put **hard caps** on nested limits.\n- Keep \u201cinclude\u201d depth shallow (e.g., max 2).\n\n### 2) Avoiding over/under-fetching pitfalls\nIntroduce a small number of \u201cview model\u201d endpoints for common screens:\n- `GET /project-summaries?limit=...` (projects + open task counts)\n- `GET /project-detail/{projectId}` (project + tasks + last 3 comments per task)\n\nThis is the REST analogue of GraphQL query flexibility, but with bounded complexity.\n\n### 3) Caching strategy\nCaching is where REST tends to win for simplicity.\n\n- **Client-side caching**: Use ETags.\n  - Return `ETag` and `Cache-Control: private, max-age=0`.\n  - Support `If-None-Match` for `GET /projects/{id}`, `GET /tasks/{id}`.\n- **Server-side caching** (optional):\n  - Cache \u201cview model\u201d responses for short TTLs (e.g., 10\u201330s) keyed by `(userId, projectId, include params)`.\n  - Invalidate on writes (task/comment changes) if you have a simple event bus; otherwise TTL is fine for internal apps.\n- **CDN caching**: Probably limited value if responses are user-scoped; still useful for public/static assets, not core API.\n\nKey disagreement point: **invalidate vs TTL**.\n- TTL is simpler and often \u201cgood enough\u201d for internal tools.\n- Invalidation is more correct but adds complexity and failure modes.\n\n### 4) Versioning\nPrefer **non-breaking evolution** + additive changes.\n- Add fields instead of changing semantics.\n- For breaking changes, use **URL versioning**: `/v1/...` and `/v2/...`.\n- Alternatively (also defensible): version by media type (`Accept: application/vnd.myapp.v1+json`). This is cleaner but more overhead.\n\nGiven your team size, I\u2019d pick **URL versioning** for clarity.\n\n### 5) Performance and database access\n- Use **cursor pagination** for lists.\n- Ensure indexes:\n  - `tasks(project_id, status, created_at)`\n  - `comments(task_id, created_at)`\n- For include endpoints, avoid N+1 by using batched queries:\n  - Fetch project\n  - Fetch tasks for project (limited)\n  - Fetch comments for those tasks with `WHERE task_id IN (...)` and window functions/limits per task if needed.\n\n### 6) Auth and permissions\nKeep authorization at the resource level:\n- `Project` membership checks.\n- `Task` inherits from project.\n- `Comment` inherits from task.\n\nThis is simpler than field-level auth in GraphQL.\n\n## If you choose GraphQL instead (a viable alternative)\nIf your frontend team is pushing for GraphQL, a safe plan is:\n- Start with a **single GraphQL gateway** in NestJS.\n- Use **DataLoader** everywhere to batch DB calls.\n- Use **persisted queries** (mobile especially) and disable arbitrary queries in production.\n- Define caching around **persisted query IDs** + variables.\n- Establish schema governance: deprecations, review, and complexity limits.\n\nThe disagreement here is mostly: do you want to pay the \u201cplatform tax\u201d now for flexibility later?\n\n## Two questions that decide this quickly\n1) Do web and mobile screens already differ significantly in the fields/relationships they need, and will that divergence increase?\n2) Do you anticipate many \u201cone-off\u201d composite views that would otherwise require many bespoke REST endpoints?\n\nIf the answer to both is \u201cyes,\u201d GraphQL becomes more compelling. Otherwise, REST + include + a few view endpoints is usually the highest-velocity, lowest-risk path.",
    "context": {
      "target_categories": [
        "disagreements"
      ],
      "difficulty": "medium",
      "rationale": "Targets disagreements: REST vs GraphQL is a context-dependent architectural choice with multiple valid solutions. The response makes a recommendation but explicitly highlights trade-offs and decision points (team/platform tax, caching/invalidation, versioning approach, include vs bespoke endpoints), inviting debate rather than asserting a single universally correct answer.",
      "source": "synthetic-discovery-generator",
      "generated_at": "2026-01-08T10:51:57.526669"
    }
  }
]